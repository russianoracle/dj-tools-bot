#!/usr/bin/env python3
"""
Convert Features Format

–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ —Ñ–∏—á–∏ –∏–∑ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
(ZoneFeatures objects) –≤ —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–π —Ñ–æ—Ä–º–∞—Ç (–æ—Ç–¥–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏).
"""

import sys
import pickle
import pandas as pd
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.training.zone_features import ZoneFeatures


def convert_features(
    input_pkl: str = "models/checkpoints/features.pkl",
    output_csv: str = "results/user_tracks_features_expanded.csv"
):
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç features.pkl –≤ —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–π CSV —Ñ–æ—Ä–º–∞—Ç.

    Args:
        input_pkl: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–º —Ñ–∏—á–∞–º
        output_csv: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç–æ–≥–æ CSV
    """
    print("=" * 80)
    print("üîÑ CONVERTING FEATURES FORMAT")
    print("=" * 80)

    input_pkl = Path(input_pkl)
    if not input_pkl.exists():
        raise FileNotFoundError(f"Features not found: {input_pkl}")

    print(f"\nüìÇ Loading features from: {input_pkl}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º features
    with open(input_pkl, 'rb') as f:
        features_df = pickle.load(f)

    print(f"‚úÖ Loaded {len(features_df)} tracks")
    print(f"   Columns: {list(features_df.columns)}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç
    if 'features_list' not in features_df.columns:
        print(f"\n‚ùå Error: 'features_list' column not found")
        print(f"   Available columns: {list(features_df.columns)}")
        return None

    print(f"\nüîß Expanding ZoneFeatures objects...")

    # –†–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–µ —Ñ–∏—á–∏
    expanded_rows = []

    for idx, row in features_df.iterrows():
        zone_features = row['features_list']

        # –ï—Å–ª–∏ —ç—Ç–æ ZoneFeatures –æ–±—ä–µ–∫—Ç, –∏–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∏—á–∏
        if isinstance(zone_features, ZoneFeatures):
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º to_vector() –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö —Ñ–∏—á–µ–π
            feature_vector = zone_features.to_vector()

            # –ù–∞–∑–≤–∞–Ω–∏—è —Ñ–∏—á–µ–π (—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –ø–æ—Ä—è–¥–∫—É –≤ to_vector)
            feature_names = [
                'tempo',
                'zero_crossing_rate',
                'rms_energy',
                'spectral_centroid',
                'spectral_rolloff',
                'energy_variance',
                'mfcc_1_mean', 'mfcc_1_std',
                'mfcc_2_mean', 'mfcc_2_std',
                'mfcc_3_mean', 'mfcc_3_std',
                'mfcc_4_mean', 'mfcc_4_std',
                'mfcc_5_mean', 'mfcc_5_std',
                'low_energy',
                'brightness',
                'drop_intensity'
            ]

            # –°–æ–∑–¥–∞—ë–º —Å–ª–æ–≤–∞—Ä—å —Å —Ñ–∏—á–∞–º–∏
            feature_dict = {name: value for name, value in zip(feature_names, feature_vector)}

            # –î–æ–±–∞–≤–ª—è–µ–º audio_path
            feature_dict['audio_path'] = row['audio_path']

            expanded_rows.append(feature_dict)
        else:
            print(f"‚ö†Ô∏è  Warning: Row {idx} has unexpected type: {type(zone_features)}")

    # –°–æ–∑–¥–∞—ë–º DataFrame —Å —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–º–∏ —Ñ–∏—á–∞–º–∏
    expanded_df = pd.DataFrame(expanded_rows)

    # –ü–µ—Ä–µ—Å—Ç–∞–≤–ª—è–µ–º audio_path –≤ –Ω–∞—á–∞–ª–æ
    cols = ['audio_path'] + [col for col in expanded_df.columns if col != 'audio_path']
    expanded_df = expanded_df[cols]

    print(f"\n‚úÖ Expansion completed!")
    print(f"   Original shape: {features_df.shape}")
    print(f"   Expanded shape: {expanded_df.shape}")
    print(f"   Feature columns: {expanded_df.shape[1] - 1}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
    output_csv = Path(output_csv)
    output_csv.parent.mkdir(parents=True, exist_ok=True)

    expanded_df.to_csv(output_csv, index=False)
    print(f"\nüíæ Saved to: {output_csv}")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\nüìä Feature statistics:")
    print(f"   Columns: {', '.join(expanded_df.columns[:6].tolist())}...")
    print(f"\n   Sample (first 3 rows):")
    print(expanded_df.head(3).iloc[:, :6].to_string())

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN
    nan_count = expanded_df.isna().sum().sum()
    if nan_count > 0:
        print(f"\n‚ö†Ô∏è  Warning: Found {nan_count} NaN values")
        nan_cols = expanded_df.columns[expanded_df.isna().any()].tolist()
        for col in nan_cols:
            nan_count_col = expanded_df[col].isna().sum()
            print(f"     - {col}: {nan_count_col} NaNs")
    else:
        print(f"\n‚úÖ No NaN values found")

    print(f"\n" + "=" * 80)
    print(f"‚úÖ CONVERSION COMPLETED!")
    print(f"=" * 80)
    print(f"\nNext step:")
    print(f"  python scripts/predict_user_tracks.py \\")
    print(f"    --features {output_csv} \\")
    print(f"    --model-dir models/arousal_valence \\")
    print(f"    --output results/user_tracks_predictions.csv")

    return expanded_df


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description='Convert features from aggregated to expanded format'
    )
    parser.add_argument(
        '--input', type=str, default='models/checkpoints/features.pkl',
        help='Input features pickle (default: models/checkpoints/features.pkl)'
    )
    parser.add_argument(
        '--output', type=str, default='results/user_tracks_features_expanded.csv',
        help='Output CSV (default: results/user_tracks_features_expanded.csv)'
    )

    args = parser.parse_args()

    try:
        convert_features(
            input_pkl=args.input,
            output_csv=args.output
        )
        return 0
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
