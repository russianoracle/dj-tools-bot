#!/usr/bin/env python3
"""
Extract Features from User Tracks

–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞—É–¥–∏–æ —Ñ–∏—á–∏ –∏–∑ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö —Ç—Ä–µ–∫–æ–≤ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ
–ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è arousal-valence —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–∞.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç ZoneTrainer —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Ñ–∏—á–µ–π.
"""

import sys
import argparse
import pandas as pd
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.training.zone_trainer import ZoneTrainer


def extract_features(test_data_path: str = "tests/test_data.txt",
                    use_cache: bool = True,
                    checkpoint_dir: str = "models/checkpoints",
                    output_path: str = "results/user_tracks_features.csv"):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–∏—á–∏ –∏–∑ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö —Ç—Ä–µ–∫–æ–≤.

    Args:
        test_data_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å–æ —Å–ø–∏—Å–∫–æ–º —Ç—Ä–µ–∫–æ–≤
        use_cache: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–µ—à (–µ—Å–ª–∏ –µ—Å—Ç—å)
        checkpoint_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –∫–µ—à–∞
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è CSV —Å —Ñ–∏—á–∞–º–∏

    Returns:
        DataFrame —Å —Ñ–∏—á–∞–º–∏
    """
    print("=" * 80)
    print("üéµ USER TRACK FEATURE EXTRACTION")
    print("=" * 80)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    test_data_path = Path(test_data_path)
    if not test_data_path.exists():
        raise FileNotFoundError(f"Test data file not found: {test_data_path}")

    print(f"\nüìÇ Input file: {test_data_path}")

    # –ü–æ–¥—Å—á–µ—Ç —Ç—Ä–µ–∫–æ–≤
    with open(test_data_path, 'r', encoding='utf-16') as f:
        lines = [line.strip() for line in f if line.strip() and not line.startswith('#')]

    print(f"üìä Total tracks to process: {len(lines)}")

    # –°–æ–∑–¥–∞–µ–º ZoneTrainer (checkpoint_manager —Å–æ–∑–¥–∞—ë—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
    print(f"\nüîß Creating ZoneTrainer with automatic caching...")
    print(f"   Cache directory: {checkpoint_dir}")

    trainer = ZoneTrainer(
        test_data_path=str(test_data_path),
        use_gpu=False,  # –ù–µ –Ω—É–∂–µ–Ω GPU –¥–ª—è feature extraction
        use_embeddings=False,
        use_music_emotion=False,
        use_fast_mode=False  # –ü–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä —Ñ–∏—á–µ–π
    )

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    print(f"\nüì• Loading training data...")

    def log_callback(level, message):
        """Callback –¥–ª—è –ª–æ–≥–æ–≤."""
        print(f"  {message}")

    def progress_callback(current, total, message):
        """Callback –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞."""
        percent = (current / total) * 100 if total > 0 else 0
        print(f"  [{current}/{total}] ({percent:.1f}%) - {message}")

    trainer.load_training_data(
        progress_callback=progress_callback,
        log_callback=log_callback
    )

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∏—á–∏ (—Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º)
    print(f"\nüéØ Extracting features...")
    print(f"   Use cache: {use_cache}")
    print(f"   Checkpoint interval: every 5 tracks")

    features_df = trainer.extract_features(
        use_cache=use_cache,
        progress_callback=progress_callback,
        log_callback=log_callback,
        checkpoint_interval=5
    )

    print(f"\n‚úÖ Feature extraction completed!")
    print(f"   Extracted features: {len(features_df)}")
    print(f"   Feature columns: {features_df.shape[1]}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    features_df.to_csv(output_path, index=False)
    print(f"\nüíæ Features saved to: {output_path}")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\nüìà Feature statistics:")
    print(f"   Columns: {', '.join(features_df.columns[:5].tolist())}...")
    print(f"\n   Sample (first 3 tracks):")
    print(features_df.head(3).to_string())

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN
    nan_count = features_df.isna().sum().sum()
    if nan_count > 0:
        print(f"\n‚ö†Ô∏è  Warning: Found {nan_count} NaN values in features")
        print(f"   Columns with NaNs:")
        nan_cols = features_df.columns[features_df.isna().any()].tolist()
        for col in nan_cols:
            nan_count_col = features_df[col].isna().sum()
            print(f"     - {col}: {nan_count_col} NaNs")
    else:
        print(f"\n‚úÖ No NaN values found")

    print(f"\n" + "=" * 80)
    print(f"‚úÖ FEATURE EXTRACTION COMPLETED!")
    print(f"=" * 80)
    print(f"\nFeatures cached to: {checkpoint_dir}/features.pkl")
    print(f"Features CSV saved to: {output_path}")
    print(f"\nNext steps:")
    print(f"  1. Apply arousal-valence regressor to predict arousal/valence")
    print(f"  2. Visualize distribution compared to DEAM dataset")

    return features_df


def main():
    parser = argparse.ArgumentParser(
        description='Extract features from user tracks for arousal-valence prediction'
    )
    parser.add_argument(
        '--test-data', type=str, default='tests/test_data.txt',
        help='Path to test data file (default: tests/test_data.txt)'
    )
    parser.add_argument(
        '--no-cache', action='store_true',
        help='Force re-extraction (ignore cache)'
    )
    parser.add_argument(
        '--checkpoint-dir', type=str, default='models/checkpoints',
        help='Checkpoint directory for caching (default: models/checkpoints)'
    )
    parser.add_argument(
        '--output', type=str, default='results/user_tracks_features.csv',
        help='Output CSV path (default: results/user_tracks_features.csv)'
    )

    args = parser.parse_args()

    try:
        extract_features(
            test_data_path=args.test_data,
            use_cache=not args.no_cache,
            checkpoint_dir=args.checkpoint_dir,
            output_path=args.output
        )
        return 0

    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
