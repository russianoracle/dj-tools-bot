"""
DEAM Dataset Trainer Extension

–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ ZoneTrainer –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å DEAM –¥–∞—Ç–∞—Å–µ—Ç–æ–º.
–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–µ–¥—Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã–µ —Ñ–∏—á–∏ –≤–º–µ—Å—Ç–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑ –∞—É–¥–∏–æ.
"""

import pandas as pd
import numpy as np
from pathlib import Path
from typing import Optional, Callable, Tuple
from sklearn.preprocessing import StandardScaler

from .zone_trainer import ZoneTrainer, ZONE_LABELS
from .zone_features import ZoneFeatures
from .checkpoint_manager import CheckpointManager
from ..utils import get_logger

logger = get_logger(__name__)


class DEAMZoneTrainer(ZoneTrainer):
    """
    –¢—Ä–µ–Ω–µ—Ä –¥–ª—è DEAM –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –ø—Ä–µ–¥—Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã–º–∏ —Ñ–∏—á–∞–º–∏.

    –û—Ç–ª–∏—á–∏—è –æ—Ç –±–∞–∑–æ–≤–æ–≥–æ ZoneTrainer:
    - –ó–∞–≥—Ä—É–∂–∞–µ—Ç –≥–æ—Ç–æ–≤—ã–µ —Ñ–∏—á–∏ –∏–∑ CSV (–Ω–µ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑ –∞—É–¥–∏–æ)
    - –†–∞–±–æ—Ç–∞–µ—Ç —Å arousal-valence –º–µ—Ç–∫–∞–º–∏
    - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–æ—Ç–æ–≤—ã–µ train/val/test splits
    """

    def __init__(self, deam_dir: str = "dataset/deam_processed"):
        """
        Initialize DEAM trainer.

        Args:
            deam_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–º–∏ DEAM –¥–∞–Ω–Ω—ã–º–∏
                     (—Å–æ–∑–¥–∞—ë—Ç—Å—è scripts/prepare_deam_dataset.py)
        """
        # –ù–µ –≤—ã–∑—ã–≤–∞–µ–º super().__init__() —Ç–∞–∫ –∫–∞–∫ –Ω–µ –Ω—É–∂–µ–Ω test_data_path
        self.deam_dir = Path(deam_dir)
        self.use_gpu = False  # –§–∏—á–∏ —É–∂–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã

        # Checkpoint manager –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –±–∞–∑–æ–≤—ã–º ZoneTrainer
        self.checkpoint_manager = CheckpointManager(checkpoint_dir="models/checkpoints")
        self.use_embeddings = False
        self.use_music_emotion = False
        self.use_fast_mode = False
        self._should_stop = False

        # Feature extractor –Ω–µ –Ω—É–∂–µ–Ω (—Ñ–∏—á–∏ —É–∂–µ –≥–æ—Ç–æ–≤—ã)
        self.feature_extractor = None

        # Data storage
        self.audio_paths = []
        self.zone_labels = []
        self.features_list = []

        # Training data (–±—É–¥—É—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω—ã –≤ load_deam_dataset)
        self.X_train = None
        self.X_val = None
        self.X_test = None
        self.y_train = None
        self.y_val = None
        self.y_test = None

        # Scaler –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        self.scaler = StandardScaler()

        # Feature names (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –±–∞–∑–æ–≤—ã–º ZoneTrainer)
        self.feature_names = None

    def load_deam_dataset(self,
                         use_precomputed_splits: bool = True,
                         test_size: float = 0.15,
                         val_size: float = 0.15,
                         random_state: int = 42,
                         progress_callback: Optional[Callable] = None,
                         log_callback: Optional[Callable] = None) -> Tuple[int, int, int]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç DEAM –¥–∞—Ç–∞—Å–µ—Ç —Å –ø—Ä–µ–¥—Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã–º–∏ —Ñ–∏—á–∞–º–∏.

        Args:
            use_precomputed_splits: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–æ—Ç–æ–≤—ã–µ splits –∏–∑ prepare_deam_dataset.py
            test_size: –†–∞–∑–º–µ—Ä test set (–µ—Å–ª–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–æ—Ç–æ–≤—ã–µ splits)
            val_size: –†–∞–∑–º–µ—Ä val set
            random_state: Random seed
            progress_callback: Callback –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            log_callback: Callback –¥–ª—è –ª–æ–≥–æ–≤

        Returns:
            Tuple (train_size, val_size, test_size)
        """
        self._log(log_callback, "INFO", f"üìÇ Loading DEAM dataset from: {self.deam_dir}")

        if not self.deam_dir.exists():
            raise FileNotFoundError(
                f"DEAM directory not found: {self.deam_dir}\n"
                f"Please run: python scripts/prepare_deam_dataset.py"
            )

        if use_precomputed_splits:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≥–æ—Ç–æ–≤—ã–µ splits
            self._log(log_callback, "INFO", "üìä Loading precomputed train/val/test splits...")

            train_df = pd.read_csv(self.deam_dir / "train.csv")
            val_df = pd.read_csv(self.deam_dir / "val.csv")
            test_df = pd.read_csv(self.deam_dir / "test.csv")

            self._log(log_callback, "INFO", f"  Train: {len(train_df)} tracks")
            self._log(log_callback, "INFO", f"  Val:   {len(val_df)} tracks")
            self._log(log_callback, "INFO", f"  Test:  {len(test_df)} tracks")

        else:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏ –¥–µ–ª–∞–µ–º split
            self._log(log_callback, "INFO", "üìä Loading complete dataset and creating splits...")
            complete_df = pd.read_csv(self.deam_dir / "deam_complete.csv")

            from sklearn.model_selection import train_test_split

            train_val, test_df = train_test_split(
                complete_df, test_size=test_size, random_state=random_state,
                stratify=complete_df['zone']
            )

            val_size_adjusted = val_size / (1 - test_size)
            train_df, val_df = train_test_split(
                train_val, test_size=val_size_adjusted, random_state=random_state,
                stratify=train_val['zone']
            )

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∏—á–∏ –∏ –º–µ—Ç–∫–∏
        self._log(log_callback, "INFO", "üîß Extracting features and labels...")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Ñ–∏—á–µ–π (–≤—Å–µ –∫—Ä–æ–º–µ —Å–ª—É–∂–µ–±–Ω—ã—Ö)
        meta_columns = ['track_id', 'audio_path', 'arousal', 'valence', 'zone',
                       'arousal_std', 'valence_std', 'success', 'error']

        # –í—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ - —ç—Ç–æ —Ñ–∏—á–∏
        all_columns = train_df.columns.tolist()
        feature_columns = [col for col in all_columns if col not in meta_columns]

        self.feature_names = feature_columns
        self._log(log_callback, "INFO", f"  Features: {len(feature_columns)}")
        self._log(log_callback, "INFO", f"  Feature list: {', '.join(feature_columns[:5])}...")

        # –ò–∑–≤–ª–µ–∫–∞–µ–º X (—Ñ–∏—á–∏) –∏ y (–∑–æ–Ω—ã)
        X_train = train_df[feature_columns].values
        X_val = val_df[feature_columns].values
        X_test = test_df[feature_columns].values

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∑–æ–Ω—ã –≤ —á–∏—Å–ª–æ–≤—ã–µ –º–µ—Ç–∫–∏
        y_train = train_df['zone'].str.lower().map(ZONE_LABELS).values
        y_val = val_df['zone'].str.lower().map(ZONE_LABELS).values
        y_test = test_df['zone'].str.lower().map(ZONE_LABELS).values

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ñ–∏—á–∏
        self._log(log_callback, "INFO", "üìè Normalizing features...")
        self.scaler.fit(X_train)

        self.X_train = self.scaler.transform(X_train)
        self.X_val = self.scaler.transform(X_val)
        self.X_test = self.scaler.transform(X_test)

        self.y_train = y_train
        self.y_val = y_val
        self.y_test = y_test

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º audio paths –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        self.audio_paths = train_df['audio_path'].tolist() if 'audio_path' in train_df.columns else []
        self.zone_labels = train_df['zone'].tolist()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self._log(log_callback, "INFO", "‚úÖ Dataset loaded successfully!")
        self._log(log_callback, "INFO", f"")
        self._log(log_callback, "INFO", f"üìà Dataset statistics:")
        self._log(log_callback, "INFO", f"  Train: {len(X_train)} samples, {X_train.shape[1]} features")
        self._log(log_callback, "INFO", f"  Val:   {len(X_val)} samples")
        self._log(log_callback, "INFO", f"  Test:  {len(X_test)} samples")

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–æ–Ω
        from collections import Counter
        train_dist = Counter(train_df['zone'])
        self._log(log_callback, "INFO", f"")
        self._log(log_callback, "INFO", f"üéØ Zone distribution (train):")
        for zone, count in sorted(train_dist.items()):
            pct = count / len(train_df) * 100
            self._log(log_callback, "INFO", f"  {zone}: {count} ({pct:.1f}%)")

        return len(X_train), len(X_val), len(X_test)

    def load_training_data(self, *args, **kwargs):
        """
        Override –±–∞–∑–æ–≤–æ–≥–æ –º–µ—Ç–æ–¥–∞ - –¥–ª—è DEAM –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ load_deam_dataset()
        """
        raise NotImplementedError(
            "For DEAM dataset, use load_deam_dataset() instead of load_training_data()"
        )

    def extract_features(self, *args, **kwargs):
        """
        Override –±–∞–∑–æ–≤–æ–≥–æ –º–µ—Ç–æ–¥–∞ - —Ñ–∏—á–∏ —É–∂–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã –≤ DEAM
        """
        raise NotImplementedError(
            "For DEAM dataset, features are pre-extracted. Use load_deam_dataset()"
        )

    def prepare_datasets(self, *args, **kwargs):
        """
        Override –±–∞–∑–æ–≤–æ–≥–æ –º–µ—Ç–æ–¥–∞ - splits —É–∂–µ –≥–æ—Ç–æ–≤—ã –≤ DEAM
        """
        raise NotImplementedError(
            "For DEAM dataset, splits are prepared in load_deam_dataset()"
        )

    def _log(self, callback: Optional[Callable], level: str, message: str):
        """Helper –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        if callback:
            callback(level, message)
        else:
            log_func = getattr(logger, level.lower(), logger.info)
            log_func(message)


def main():
    """–¢–µ—Å—Ç–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import sys

    logging.basicConfig(level=logging.INFO)

    try:
        trainer = DEAMZoneTrainer()
        train_size, val_size, test_size = trainer.load_deam_dataset()

        print(f"\n‚úÖ Successfully loaded DEAM dataset!")
        print(f"  Train: {train_size}")
        print(f"  Val:   {val_size}")
        print(f"  Test:  {test_size}")
        print(f"  Features: {trainer.X_train.shape[1]}")

        return 0

    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    import logging
    sys.exit(main())
